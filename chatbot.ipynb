{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\OneDrive\\Documents\\Desktop\\RAG\\RAG\\newvenv\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "#import all the necessary packages \n",
    "from langchain import PromptTemplate\n",
    "import os\n",
    "import pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from pinecone import ServerlessSpec,Pinecone\n",
    "# from langchain.document_loaders import DirectoryLoader,PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\OneDrive\\Documents\\Desktop\\RAG\\RAG\\newvenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#instantiate the openai \n",
    "llm = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the openai embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_book(path):\n",
    "    loader = DirectoryLoader(\n",
    "        path,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    \n",
    "    document = loader.load()\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = load_book('C:/Users/Hp/OneDrive/Documents/Desktop/RAG/RAG/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into chunks \n",
    "def split_text(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap = 20\n",
    "    )\n",
    "    \n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = split_text(document)\n",
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PINECONE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the environment variable \n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "pinecone_env_key = os.getenv('PINECONE_ENV_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pincone integration\n",
    "pc = Pinecone(\n",
    "    api_key=pinecone_api_key\n",
    ")\n",
    "\n",
    "index_name = 'pdfreader2' #specify the index name where we have stored the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you do not have index you can run this code else skip\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension=1536,\n",
    "        spec=ServerlessSpec(\n",
    "            region=pinecone_env_key,\n",
    "            cloud='aws'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create embedding for your text chunks\n",
    "\n",
    "embedding_list = []\n",
    "def create_embedding(query):\n",
    "    embedding_list.append(embedding.embed_query(query))\n",
    "    \n",
    "for text in text_chunks:\n",
    "    create_embedding(text.page_content)\n",
    "\n",
    "len(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_list = [{\"text\": text_chunks[i].page_content } for i in range(len(embedding_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_upsert = [\n",
    "    {\n",
    "        'id': f\"id-{i}\",\n",
    "        'values': embedding,\n",
    "        'metadata': metadata_list[i]  # Include metadata\n",
    "    }\n",
    "    for i, embedding in enumerate(embedding_list)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(iterable, batch_size=200):\n",
    "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    chunk = tuple(itertools.islice(it, batch_size))\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = tuple(itertools.islice(it, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert data with 200 vectors per upsert request\n",
    "for ids_vectors_chunk in chunks(data_to_upsert, batch_size=200):\n",
    "    index.upsert(vectors=ids_vectors_chunk) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables= ['answer','question'],\n",
    "    template = '''You are given a answer for a question. Refine the given answer based on the question and give relevent answer to \n",
    "    the question. Give a general answer \n",
    "    answer: {answer},\n",
    "    question: {question}\n",
    "    \n",
    "    Please answer clearly\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is Incept AI'\n",
    "\n",
    "vector = embedding.embed_query(query)\n",
    "\n",
    "query_result = index.query(\n",
    "    vector = vector,\n",
    "    top_k=3,\n",
    "    include_values = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [], 'namespace': '', 'usage': {'read_units': 1}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_id = [query_result.matches[0].id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_results = index.fetch(ids=matched_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = metadata_results.vectors['id-1'].metadata['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    prompt = prompt,\n",
    "    llm = llm\n",
    ")\n",
    "\n",
    "chain.run(answer=result,question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_function():\n",
    "    print('This is the dummy function from notebook')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
